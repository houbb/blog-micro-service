---
title: é«˜å¯ç”¨æ€§ç¯å¢ƒä¸­çš„é…ç½®ç®¡ç†ç­–ç•¥ï¼šæ„å»ºç¨³å®šå¯é çš„é…ç½®ç®¡ç†ä½“ç³»
date: 2025-08-31
categories: [Configuration Management]
tags: [microservice-configuration-management]
published: true
---

# 15.1 é«˜å¯ç”¨æ€§ç¯å¢ƒä¸­çš„é…ç½®ç®¡ç†ç­–ç•¥

åœ¨æ„å»ºé«˜å¯ç”¨æ€§ç³»ç»Ÿæ—¶ï¼Œé…ç½®ç®¡ç†ç­–ç•¥çš„åˆç†è®¾è®¡æ˜¯ç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œçš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚é«˜å¯ç”¨æ€§ç¯å¢ƒå¯¹é…ç½®ç®¡ç†æå‡ºäº†æ›´é«˜çš„è¦æ±‚ï¼Œä¸ä»…éœ€è¦ä¿è¯é…ç½®çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ï¼Œè¿˜éœ€è¦åœ¨å„ç§æ•…éšœåœºæ™¯ä¸‹ç¡®ä¿é…ç½®æœåŠ¡çš„æŒç»­å¯ç”¨ã€‚æœ¬èŠ‚å°†æ·±å…¥æ¢è®¨é«˜å¯ç”¨æ€§æ¶æ„è®¾è®¡åŸåˆ™ã€é…ç½®ç®¡ç†ç»„ä»¶çš„é«˜å¯ç”¨éƒ¨ç½²ã€è´Ÿè½½å‡è¡¡ä¸æ•…éšœè½¬ç§»æœºåˆ¶ï¼Œä»¥åŠå¥åº·æ£€æŸ¥ä¸è‡ªåŠ¨æ¢å¤ç­–ç•¥ç­‰å…³é”®ä¸»é¢˜ã€‚

## é«˜å¯ç”¨æ€§æ¶æ„è®¾è®¡åŸåˆ™

è®¾è®¡é«˜å¯ç”¨æ€§é…ç½®ç®¡ç†æ¶æ„éœ€è¦éµå¾ªä¸€ç³»åˆ—æ ¸å¿ƒåŸåˆ™ï¼Œè¿™äº›åŸåˆ™æŒ‡å¯¼ç€æ•´ä¸ªç³»ç»Ÿçš„æ¶æ„å†³ç­–å’Œå®ç°æ–¹æ¡ˆã€‚

### 1. å†—ä½™è®¾è®¡åŸåˆ™

å†—ä½™æ˜¯å®ç°é«˜å¯ç”¨æ€§çš„åŸºç¡€ï¼Œé€šè¿‡æä¾›é¢å¤–çš„ç»„ä»¶å’Œè·¯å¾„æ¥æ¶ˆé™¤å•ç‚¹æ•…éšœã€‚

```yaml
# ha-architecture-principles.yaml
---
design_principles:
  redundancy:
    description: "å†—ä½™è®¾è®¡åŸåˆ™"
    key_points:
      - "æ¶ˆé™¤å•ç‚¹æ•…éšœ"
      - "æä¾›å¤‡ç”¨ç»„ä»¶å’Œè·¯å¾„"
      - "å®ç°æ•…éšœè‡ªåŠ¨åˆ‡æ¢"
      - "ç¡®ä¿æ•°æ®å¤šå‰¯æœ¬å­˜å‚¨"
    implementation_patterns:
      - pattern: "ä¸»å¤‡æ¨¡å¼"
        description: "ä¸€ä¸ªä¸»èŠ‚ç‚¹å¤„ç†è¯·æ±‚ï¼Œå¤šä¸ªå¤‡ç”¨èŠ‚ç‚¹å¾…å‘½"
        use_cases: ["æ•°æ®åº“é…ç½®å­˜å‚¨", "é…ç½®ç®¡ç†æœåŠ¡å™¨"]
        
      - pattern: "ä¸»ä¸»æ¨¡å¼"
        description: "å¤šä¸ªèŠ‚ç‚¹åŒæ—¶å¤„ç†è¯·æ±‚ï¼Œè´Ÿè½½åˆ†æ‹…"
        use_cases: ["åˆ†å¸ƒå¼é…ç½®å­˜å‚¨", "è´Ÿè½½å‡è¡¡é…ç½®æœåŠ¡"]
        
      - pattern: "é›†ç¾¤æ¨¡å¼"
        description: "å¤šä¸ªèŠ‚ç‚¹ç»„æˆé›†ç¾¤ï¼ŒååŒå·¥ä½œ"
        use_cases: ["Kubernetes ConfigMap", "Consulé›†ç¾¤"]

  fault_isolation:
    description: "æ•…éšœéš”ç¦»åŸåˆ™"
    key_points:
      - "å°†ç³»ç»Ÿåˆ’åˆ†ä¸ºç‹¬ç«‹çš„æ•…éšœåŸŸ"
      - "é™åˆ¶æ•…éšœå½±å“èŒƒå›´"
      - "å®ç°æ•…éšœå¿«é€Ÿå®šä½"
      - "ç¡®ä¿æ•…éšœä¸å½±å“å…¶ä»–ç»„ä»¶"
    implementation_strategies:
      - "å¾®æœåŠ¡æ¶æ„éš”ç¦»"
      - "ç½‘ç»œåˆ†åŒºéš”ç¦»"
      - "æ•°æ®éš”ç¦»"
      - "èµ„æºé…ç½®éš”ç¦»"

  graceful_degradation:
    description: "ä¼˜é›…é™çº§åŸåˆ™"
    key_points:
      - "åœ¨éƒ¨åˆ†ç»„ä»¶å¤±æ•ˆæ—¶ä¿æŒæ ¸å¿ƒåŠŸèƒ½"
      - "æä¾›é™çº§æœåŠ¡çº§åˆ«"
      - "ç¡®ä¿ç”¨æˆ·ä½“éªŒä¸å—ä¸¥é‡å½±å“"
      - "æ”¯æŒæ‰‹åŠ¨å’Œè‡ªåŠ¨é™çº§"
    degradation_levels:
      - level: "å®Œå…¨åŠŸèƒ½"
        description: "æ‰€æœ‰åŠŸèƒ½æ­£å¸¸è¿è¡Œ"
        
      - level: "éƒ¨åˆ†åŠŸèƒ½å—é™"
        description: "éæ ¸å¿ƒåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨"
        
      - level: "åªè¯»æ¨¡å¼"
        description: "ä»…æä¾›é…ç½®è¯»å–ï¼Œç¦æ­¢å†™å…¥æ“ä½œ"
        
      - level: "ç¼“å­˜æ¨¡å¼"
        description: "ä½¿ç”¨æœ¬åœ°ç¼“å­˜æä¾›æœåŠ¡"
```

### 2. é«˜å¯ç”¨æ€§é…ç½®ç®¡ç†æ¶æ„

```python
# ha-config-architecture.py
import threading
import time
import json
from typing import Dict, List, Any
from datetime import datetime

class HighAvailabilityConfigManager:
    def __init__(self, cluster_nodes: List[str], replication_factor: int = 3):
        self.cluster_nodes = cluster_nodes
        self.replication_factor = replication_factor
        self.node_status = {node: 'healthy' for node in cluster_nodes}
        self.config_store = {}
        self.lock = threading.Lock()
        self.heartbeat_interval = 5  # å¿ƒè·³é—´éš”ï¼ˆç§’ï¼‰
        self.failure_threshold = 3    # æ•…éšœé˜ˆå€¼
        
    def start_heartbeat_monitoring(self):
        """å¯åŠ¨å¿ƒè·³ç›‘æ§"""
        def heartbeat_loop():
            while True:
                self.send_heartbeats()
                self.check_node_health()
                time.sleep(self.heartbeat_interval)
                
        heartbeat_thread = threading.Thread(target=heartbeat_loop, daemon=True)
        heartbeat_thread.start()
        print("âœ… Heartbeat monitoring started")
        
    def send_heartbeats(self):
        """å‘é€å¿ƒè·³ä¿¡å·"""
        for node in self.cluster_nodes:
            try:
                # æ¨¡æ‹Ÿå‘é€å¿ƒè·³
                status = self.check_node_status(node)
                with self.lock:
                    self.node_status[node] = status
            except Exception as e:
                print(f"âŒ Heartbeat failed for node {node}: {e}")
                
    def check_node_status(self, node: str) -> str:
        """æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€"""
        # æ¨¡æ‹ŸèŠ‚ç‚¹çŠ¶æ€æ£€æŸ¥
        # å®é™…åº”ç”¨ä¸­åº”è¯¥é€šè¿‡ç½‘ç»œè¯·æ±‚æ£€æŸ¥èŠ‚ç‚¹å¥åº·çŠ¶æ€
        import random
        return 'healthy' if random.random() > 0.1 else 'unhealthy'
        
    def check_node_health(self):
        """æ£€æŸ¥èŠ‚ç‚¹å¥åº·çŠ¶æ€"""
        unhealthy_nodes = []
        with self.lock:
            for node, status in self.node_status.items():
                if status != 'healthy':
                    unhealthy_nodes.append(node)
                    
        if unhealthy_nodes:
            print(f"âš ï¸ Unhealthy nodes detected: {unhealthy_nodes}")
            self.handle_node_failures(unhealthy_nodes)
            
    def handle_node_failures(self, failed_nodes: List[str]):
        """å¤„ç†èŠ‚ç‚¹æ•…éšœ"""
        for node in failed_nodes:
            print(f"ğŸ”„ Handling failure for node {node}")
            # é‡æ–°åˆ†é…è¯¥èŠ‚ç‚¹çš„é…ç½®æ•°æ®
            self.reallocate_node_data(node)
            
    def reallocate_node_data(self, failed_node: str):
        """é‡æ–°åˆ†é…èŠ‚ç‚¹æ•°æ®"""
        print(f"ğŸ”„ Reallocating data from failed node {failed_node}")
        # è¿™é‡Œåº”è¯¥å®ç°æ•°æ®é‡æ–°åˆ†é…é€»è¾‘
        # å®é™…åº”ç”¨ä¸­éœ€è¦å°†æ•°æ®å¤åˆ¶åˆ°å…¶ä»–å¥åº·èŠ‚ç‚¹
        
    def get_config(self, key: str) -> Any:
        """è·å–é…ç½®å€¼"""
        with self.lock:
            return self.config_store.get(key)
            
    def set_config(self, key: str, value: Any) -> bool:
        """è®¾ç½®é…ç½®å€¼"""
        try:
            # åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè®¾ç½®é…ç½®
            success_count = 0
            for node in self.get_healthy_nodes()[:self.replication_factor]:
                if self.set_config_on_node(node, key, value):
                    success_count += 1
                    
            # ç¡®ä¿å¤§å¤šæ•°èŠ‚ç‚¹å†™å…¥æˆåŠŸ
            if success_count >= (self.replication_factor // 2) + 1:
                with self.lock:
                    self.config_store[key] = value
                print(f"âœ… Configuration {key} set successfully")
                return True
            else:
                print(f"âŒ Failed to set configuration {key}")
                return False
                
        except Exception as e:
            print(f"âŒ Error setting configuration {key}: {e}")
            return False
            
    def set_config_on_node(self, node: str, key: str, value: Any) -> bool:
        """åœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Šè®¾ç½®é…ç½®"""
        # æ¨¡æ‹Ÿåœ¨èŠ‚ç‚¹ä¸Šè®¾ç½®é…ç½®
        print(f"ğŸ“ Setting config {key} on node {node}")
        return True
        
    def get_healthy_nodes(self) -> List[str]:
        """è·å–å¥åº·èŠ‚ç‚¹åˆ—è¡¨"""
        with self.lock:
            return [node for node, status in self.node_status.items() 
                   if status == 'healthy']
            
    def get_cluster_status(self) -> Dict[str, Any]:
        """è·å–é›†ç¾¤çŠ¶æ€"""
        with self.lock:
            healthy_count = sum(1 for status in self.node_status.values() 
                              if status == 'healthy')
            total_count = len(self.node_status)
            
            return {
                'timestamp': datetime.now().isoformat(),
                'total_nodes': total_count,
                'healthy_nodes': healthy_count,
                'unhealthy_nodes': total_count - healthy_count,
                'node_status': self.node_status.copy(),
                'config_count': len(self.config_store)
            }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºé«˜å¯ç”¨é…ç½®ç®¡ç†å™¨
    cluster_nodes = ['node1.example.com', 'node2.example.com', 'node3.example.com']
    ha_config_manager = HighAvailabilityConfigManager(cluster_nodes)
    
    # å¯åŠ¨å¿ƒè·³ç›‘æ§
    ha_config_manager.start_heartbeat_monitoring()
    
    # è®¾ç½®ä¸€äº›é…ç½®
    ha_config_manager.set_config('database.host', 'db.example.com')
    ha_config_manager.set_config('cache.redis.host', 'redis.example.com')
    
    # è·å–é…ç½®
    db_host = ha_config_manager.get_config('database.host')
    print(f"Database host: {db_host}")
    
    # è·å–é›†ç¾¤çŠ¶æ€
    status = ha_config_manager.get_cluster_status()
    print(f"Cluster status: {json.dumps(status, indent=2)}")
    
    # ä¿æŒç¨‹åºè¿è¡Œä»¥è§‚å¯Ÿå¿ƒè·³ç›‘æ§
    try:
        while True:
            time.sleep(10)
    except KeyboardInterrupt:
        print("ğŸ‘‹ Shutting down...")
```

## é…ç½®ç®¡ç†ç»„ä»¶çš„é«˜å¯ç”¨éƒ¨ç½²

å®ç°é…ç½®ç®¡ç†ç»„ä»¶çš„é«˜å¯ç”¨éƒ¨ç½²æ˜¯ç¡®ä¿é…ç½®æœåŠ¡æŒç»­å¯ç”¨çš„å…³é”®æ­¥éª¤ã€‚

### 1. é…ç½®å­˜å‚¨é«˜å¯ç”¨éƒ¨ç½²

```bash
# ha-config-storage-deployment.sh

# é«˜å¯ç”¨é…ç½®å­˜å‚¨éƒ¨ç½²è„šæœ¬
deploy_ha_config_storage() {
    local storage_type=${1:-"etcd"}
    local cluster_size=${2:-3}
    
    echo "Deploying HA configuration storage: $storage_type with $cluster_size nodes"
    
    case "$storage_type" in
        "etcd")
            deploy_etcd_cluster "$cluster_size"
            ;;
        "consul")
            deploy_consul_cluster "$cluster_size"
            ;;
        "zookeeper")
            deploy_zookeeper_cluster "$cluster_size"
            ;;
        *)
            echo "Unsupported storage type: $storage_type"
            return 1
            ;;
    esac
    
    # éªŒè¯éƒ¨ç½²
    verify_storage_deployment "$storage_type"
    
    echo "HA configuration storage deployment completed"
}

# éƒ¨ç½²etcdé›†ç¾¤
deploy_etcd_cluster() {
    local cluster_size=$1
    echo "Deploying etcd cluster with $cluster_size nodes"
    
    # åˆ›å»ºetcdé…ç½®
    create_etcd_config "$cluster_size"
    
    # éƒ¨ç½²etcdèŠ‚ç‚¹
    for i in $(seq 1 $cluster_size); do
        echo "Deploying etcd node $i"
        deploy_etcd_node $i "$cluster_size"
    done
    
    # ç­‰å¾…é›†ç¾¤ç¨³å®š
    wait_for_etcd_cluster_ready "$cluster_size"
    
    # éªŒè¯é›†ç¾¤çŠ¶æ€
    verify_etcd_cluster_status
}

# åˆ›å»ºetcdé…ç½®
create_etcd_config() {
    local cluster_size=$1
    local cluster_tokens=()
    
    # ç”Ÿæˆé›†ç¾¤ä»¤ç‰Œ
    for i in $(seq 1 $cluster_size); do
        cluster_tokens+=("etcd-node-$i=http://etcd-node-$i:2380")
    done
    
    local cluster_state=$(IFS=','; echo "${cluster_tokens[*]}")
    
    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºé…ç½®
    for i in $(seq 1 $cluster_size); do
        cat > /tmp/etcd-config-node-$i.yaml << EOF
name: 'etcd-node-$i'
data-dir: '/var/lib/etcd'
listen-peer-urls: 'http://0.0.0.0:2380'
listen-client-urls: 'http://0.0.0.0:2379'
initial-advertise-peer-urls: 'http://etcd-node-$i:2380'
advertise-client-urls: 'http://etcd-node-$i:2379'
initial-cluster: '$cluster_state'
initial-cluster-state: 'new'
initial-cluster-token: 'etcd-cluster-1'
EOF
    done
}

# éƒ¨ç½²å•ä¸ªetcdèŠ‚ç‚¹
deploy_etcd_node() {
    local node_id=$1
    local cluster_size=$2
    
    echo "Deploying etcd node $node_id"
    
    # ä½¿ç”¨Dockeréƒ¨ç½²etcdèŠ‚ç‚¹
    docker run -d \
        --name etcd-node-$node_id \
        --network host \
        -v /tmp/etcd-config-node-$node_id.yaml:/etc/etcd/etcd.conf.yml \
        -v /var/lib/etcd:/var/lib/etcd \
        quay.io/coreos/etcd:v3.5.0 \
        etcd \
        --config-file=/etc/etcd/etcd.conf.yml
        
    echo "âœ… etcd node $node_id deployed"
}

# ç­‰å¾…etcdé›†ç¾¤å°±ç»ª
wait_for_etcd_cluster_ready() {
    local cluster_size=$1
    local timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
    local interval=10
    local elapsed=0
    
    echo "Waiting for etcd cluster to be ready..."
    
    while [ $elapsed -lt $timeout ]; do
        if check_etcd_cluster_health; then
            echo "âœ… etcd cluster is ready"
            return 0
        fi
        
        echo "etcd cluster not ready, waiting $interval seconds..."
        sleep $interval
        elapsed=$((elapsed + interval))
    done
    
    echo "âŒ etcd cluster failed to become ready within timeout"
    return 1
}

# æ£€æŸ¥etcdé›†ç¾¤å¥åº·çŠ¶æ€
check_etcd_cluster_health() {
    # æ£€æŸ¥é›†ç¾¤æˆå‘˜
    if docker exec etcd-node-1 etcdctl member list >/dev/null 2>&1; then
        # æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
        if docker exec etcd-node-1 etcdctl endpoint health >/dev/null 2>&1; then
            return 0
        fi
    fi
    return 1
}

# éªŒè¯etcdé›†ç¾¤çŠ¶æ€
verify_etcd_cluster_status() {
    echo "Verifying etcd cluster status"
    
    # æ˜¾ç¤ºé›†ç¾¤æˆå‘˜
    echo "Cluster members:"
    docker exec etcd-node-1 etcdctl member list
    
    # æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
    echo "Cluster status:"
    docker exec etcd-node-1 etcdctl endpoint status --write-out=table
    
    echo "âœ… etcd cluster verification completed"
}

# éƒ¨ç½²Consulé›†ç¾¤
deploy_consul_cluster() {
    local cluster_size=$1
    echo "Deploying Consul cluster with $cluster_size nodes"
    
    # åˆ›å»ºConsulé…ç½®
    create_consul_config "$cluster_size"
    
    # éƒ¨ç½²ConsulèŠ‚ç‚¹
    for i in $(seq 1 $cluster_size); do
        echo "Deploying Consul node $i"
        deploy_consul_node $i "$cluster_size"
    done
    
    # ç­‰å¾…é›†ç¾¤ç¨³å®š
    wait_for_consul_cluster_ready "$cluster_size"
    
    # éªŒè¯é›†ç¾¤çŠ¶æ€
    verify_consul_cluster_status
}

# åˆ›å»ºConsulé…ç½®
create_consul_config() {
    local cluster_size=$1
    
    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºé…ç½®
    for i in $(seq 1 $cluster_size); do
        cat > /tmp/consul-config-node-$i.json << EOF
{
    "datacenter": "dc1",
    "data_dir": "/var/lib/consul",
    "log_level": "INFO",
    "node_name": "consul-node-$i",
    "server": true,
    "ui": true,
    "bind_addr": "0.0.0.0",
    "client_addr": "0.0.0.0",
    "bootstrap_expect": $cluster_size,
    "retry_join": [
        "consul-node-1",
        "consul-node-2",
        "consul-node-3"
    ],
    "ports": {
        "dns": 8600,
        "http": 8500,
        "https": 8501,
        "serf_lan": 8301,
        "serf_wan": 8302,
        "server": 8300
    }
}
EOF
    done
}

# éƒ¨ç½²å•ä¸ªConsulèŠ‚ç‚¹
deploy_consul_node() {
    local node_id=$1
    local cluster_size=$2
    
    echo "Deploying Consul node $node_id"
    
    # ä½¿ç”¨Dockeréƒ¨ç½²ConsulèŠ‚ç‚¹
    docker run -d \
        --name consul-node-$node_id \
        --network host \
        -v /tmp/consul-config-node-$node_id.json:/consul/config/consul.json \
        -v /var/lib/consul:/var/lib/consul \
        hashicorp/consul:1.10.0 \
        agent -config-file=/consul/config/consul.json
        
    echo "âœ… Consul node $node_id deployed"
}

# ç­‰å¾…Consulé›†ç¾¤å°±ç»ª
wait_for_consul_cluster_ready() {
    local cluster_size=$1
    local timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
    local interval=10
    local elapsed=0
    
    echo "Waiting for Consul cluster to be ready..."
    
    while [ $elapsed -lt $timeout ]; do
        if check_consul_cluster_health; then
            echo "âœ… Consul cluster is ready"
            return 0
        fi
        
        echo "Consul cluster not ready, waiting $interval seconds..."
        sleep $interval
        elapsed=$((elapsed + interval))
    done
    
    echo "âŒ Consul cluster failed to become ready within timeout"
    return 1
}

# æ£€æŸ¥Consulé›†ç¾¤å¥åº·çŠ¶æ€
check_consul_cluster_health() {
    # æ£€æŸ¥é›†ç¾¤æˆå‘˜
    if docker exec consul-node-1 consul members >/dev/null 2>&1; then
        # æ£€æŸ¥é¢†å¯¼é€‰ä¸¾
        if docker exec consul-node-1 consul operator raft list-peers >/dev/null 2>&1; then
            return 0
        fi
    fi
    return 1
}

# éªŒè¯Consulé›†ç¾¤çŠ¶æ€
verify_consul_cluster_status() {
    echo "Verifying Consul cluster status"
    
    # æ˜¾ç¤ºé›†ç¾¤æˆå‘˜
    echo "Cluster members:"
    docker exec consul-node-1 consul members
    
    # æ˜¾ç¤ºRaftçŠ¶æ€
    echo "Raft status:"
    docker exec consul-node-1 consul operator raft list-peers
    
    echo "âœ… Consul cluster verification completed"
}

# éªŒè¯å­˜å‚¨éƒ¨ç½²
verify_storage_deployment() {
    local storage_type=$1
    
    echo "Verifying $storage_type deployment"
    
    case "$storage_type" in
        "etcd")
            verify_etcd_deployment
            ;;
        "consul")
            verify_consul_deployment
            ;;
        "zookeeper")
            verify_zookeeper_deployment
            ;;
    esac
}

# éªŒè¯etcdéƒ¨ç½²
verify_etcd_deployment() {
    echo "Verifying etcd deployment"
    
    # æ£€æŸ¥èŠ‚ç‚¹è¿è¡ŒçŠ¶æ€
    for container in $(docker ps --filter "name=etcd-node-" --format "{{.Names}}"); do
        if docker inspect $container >/dev/null 2>&1; then
            echo "âœ… $container is running"
        else
            echo "âŒ $container is not running"
            return 1
        fi
    done
    
    # æ£€æŸ¥é›†ç¾¤å¥åº·
    if check_etcd_cluster_health; then
        echo "âœ… etcd cluster is healthy"
    else
        echo "âŒ etcd cluster is unhealthy"
        return 1
    fi
}

# ä½¿ç”¨ç¤ºä¾‹
# deploy_ha_config_storage "etcd" 3
```

### 2. é…ç½®æœåŠ¡é«˜å¯ç”¨éƒ¨ç½²

```yaml
# ha-config-service-deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-service-ha
  labels:
    app: config-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: config-service
  template:
    metadata:
      labels:
        app: config-service
    spec:
      containers:
      - name: config-service
        image: mycompany/config-service:latest
        ports:
        - containerPort: 8080
        env:
        - name: STORAGE_TYPE
          value: "etcd"
        - name: ETCD_ENDPOINTS
          value: "etcd-node-1:2379,etcd-node-2:2379,etcd-node-3:2379"
        - name: CLUSTER_MODE
          value: "true"
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: config-service
  labels:
    app: config-service
spec:
  selector:
    app: config-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: config-service-external
  labels:
    app: config-service
spec:
  selector:
    app: config-service
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer
```

## è´Ÿè½½å‡è¡¡ä¸æ•…éšœè½¬ç§»æœºåˆ¶

æœ‰æ•ˆçš„è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»æœºåˆ¶æ˜¯å®ç°é«˜å¯ç”¨é…ç½®ç®¡ç†çš„å…³é”®ã€‚

### 1. è´Ÿè½½å‡è¡¡ç­–ç•¥

```python
# load-balancer.py
import random
import time
from typing import List, Dict, Any
from enum import Enum

class LoadBalancingStrategy(Enum):
    ROUND_ROBIN = "round_robin"
    RANDOM = "random"
    LEAST_CONNECTIONS = "least_connections"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"

class ConfigServiceLoadBalancer:
    def __init__(self, servers: List[Dict[str, Any]], strategy: LoadBalancingStrategy = LoadBalancingStrategy.ROUND_ROBIN):
        self.servers = servers
        self.strategy = strategy
        self.index = 0
        self.connection_counts = {server['host']: 0 for server in servers}
        self.failure_counts = {server['host']: 0 for server in servers}
        self.failure_threshold = 3
        self.cooldown_period = 60  # 60ç§’å†·å´æœŸ
        self.last_failure_time = {server['host']: 0 for server in servers}
        
    def get_healthy_server(self) -> Dict[str, Any]:
        """è·å–å¥åº·çš„æœåŠ¡å™¨"""
        healthy_servers = self.get_healthy_servers()
        
        if not healthy_servers:
            raise Exception("No healthy servers available")
            
        return self.select_server(healthy_servers)
        
    def get_healthy_servers(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¥åº·æœåŠ¡å™¨"""
        current_time = time.time()
        healthy_servers = []
        
        for server in self.servers:
            host = server['host']
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸ
            if current_time - self.last_failure_time[host] < self.cooldown_period:
                continue
                
            # æ£€æŸ¥æ•…éšœæ¬¡æ•°
            if self.failure_counts[host] < self.failure_threshold:
                healthy_servers.append(server)
                
        return healthy_servers
        
    def select_server(self, servers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ ¹æ®ç­–ç•¥é€‰æ‹©æœåŠ¡å™¨"""
        if self.strategy == LoadBalancingStrategy.ROUND_ROBIN:
            return self.round_robin_select(servers)
        elif self.strategy == LoadBalancingStrategy.RANDOM:
            return self.random_select(servers)
        elif self.strategy == LoadBalancingStrategy.LEAST_CONNECTIONS:
            return self.least_connections_select(servers)
        elif self.strategy == LoadBalancingStrategy.WEIGHTED_ROUND_ROBIN:
            return self.weighted_round_robin_select(servers)
        else:
            return servers[0]  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
            
    def round_robin_select(self, servers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è½®è¯¢é€‰æ‹©"""
        server = servers[self.index % len(servers)]
        self.index = (self.index + 1) % len(servers)
        return server
        
    def random_select(self, servers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """éšæœºé€‰æ‹©"""
        return random.choice(servers)
        
    def least_connections_select(self, servers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æœ€å°‘è¿æ¥é€‰æ‹©"""
        min_connections = float('inf')
        selected_server = servers[0]
        
        for server in servers:
            connections = self.connection_counts.get(server['host'], 0)
            if connections < min_connections:
                min_connections = connections
                selected_server = server
                
        return selected_server
        
    def weighted_round_robin_select(self, servers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŠ æƒè½®è¯¢é€‰æ‹©"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„æƒé‡è®¡ç®—
        return self.round_robin_select(servers)
        
    def mark_server_success(self, host: str):
        """æ ‡è®°æœåŠ¡å™¨è¯·æ±‚æˆåŠŸ"""
        self.connection_counts[host] = max(0, self.connection_counts[host] - 1)
        self.failure_counts[host] = 0  # é‡ç½®æ•…éšœè®¡æ•°
        
    def mark_server_failure(self, host: str):
        """æ ‡è®°æœåŠ¡å™¨è¯·æ±‚å¤±è´¥"""
        self.failure_counts[host] += 1
        self.last_failure_time[host] = time.time()
        print(f"âŒ Marked server {host} as failed (count: {self.failure_counts[host]})")
        
    def get_server_stats(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_servers': len(self.servers),
            'healthy_servers': len(self.get_healthy_servers()),
            'connection_counts': self.connection_counts.copy(),
            'failure_counts': self.failure_counts.copy(),
            'strategy': self.strategy.value
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®æœåŠ¡å™¨åˆ—è¡¨
    servers = [
        {'host': 'config1.example.com', 'port': 8080, 'weight': 1},
        {'host': 'config2.example.com', 'port': 8080, 'weight': 1},
        {'host': 'config3.example.com', 'port': 8080, 'weight': 1}
    ]
    
    # åˆ›å»ºè´Ÿè½½å‡è¡¡å™¨
    lb = ConfigServiceLoadBalancer(servers, LoadBalancingStrategy.ROUND_ROBIN)
    
    # æ¨¡æ‹Ÿè¯·æ±‚åˆ†å‘
    for i in range(10):
        try:
            server = lb.get_healthy_server()
            print(f"Request {i+1}: Selected server {server['host']}")
            
            # æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†
            import random
            if random.random() < 0.2:  # 20%å¤±è´¥ç‡
                lb.mark_server_failure(server['host'])
                print(f"âŒ Request {i+1} failed on {server['host']}")
            else:
                lb.mark_server_success(server['host'])
                print(f"âœ… Request {i+1} succeeded on {server['host']}")
                
        except Exception as e:
            print(f"âŒ No healthy servers available: {e}")
            
        time.sleep(1)
        
    # æ˜¾ç¤ºæœåŠ¡å™¨ç»Ÿè®¡
    stats = lb.get_server_stats()
    print(f"\nServer Statistics: {stats}")
```

### 2. æ•…éšœè½¬ç§»å®ç°

```bash
# failover-mechanism.sh

# æ•…éšœè½¬ç§»æœºåˆ¶è„šæœ¬
implement_failover_mechanism() {
    local service_name=${1:-"config-service"}
    
    echo "Implementing failover mechanism for $service_name"
    
    # 1. é…ç½®å¥åº·æ£€æŸ¥
    echo "Step 1: Configuring health checks"
    configure_health_checks "$service_name"
    
    # 2. å®æ–½æ•…éšœæ£€æµ‹
    echo "Step 2: Implementing failure detection"
    implement_failure_detection "$service_name"
    
    # 3. é…ç½®æ•…éšœè½¬ç§»
    echo "Step 3: Configuring failover"
    configure_failover "$service_name"
    
    # 4. è®¾ç½®æ¢å¤æœºåˆ¶
    echo "Step 4: Setting up recovery mechanisms"
    setup_recovery_mechanisms "$service_name"
    
    # 5. éªŒè¯æ•…éšœè½¬ç§»
    echo "Step 5: Verifying failover mechanism"
    verify_failover_mechanism "$service_name"
    
    echo "Failover mechanism implementation completed"
}

# é…ç½®å¥åº·æ£€æŸ¥
configure_health_checks() {
    local service_name=$1
    
    echo "Configuring health checks for $service_name"
    
    # åˆ›å»ºå¥åº·æ£€æŸ¥è„šæœ¬
    cat > /usr/local/bin/health-check-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
HEALTH_ENDPOINT="$2"
TIMEOUT="${3:-10}"

# æ£€æŸ¥æœåŠ¡è¿›ç¨‹æ˜¯å¦å­˜åœ¨
if ! pgrep -f "$SERVICE_NAME" > /dev/null; then
    echo "âŒ Service process not found"
    exit 1
fi

# æ£€æŸ¥å¥åº·ç«¯ç‚¹
if curl -f -s --max-time $TIMEOUT "$HEALTH_ENDPOINT" > /dev/null; then
    echo "âœ… Service is healthy"
    exit 0
else
    echo "âŒ Service health check failed"
    exit 1
fi
EOF

    chmod +x /usr/local/bin/health-check-$service_name.sh
    
    # é…ç½®ç³»ç»Ÿçº§å¥åº·æ£€æŸ¥
    cat > /etc/systemd/system/$service_name-healthcheck.service << EOF
[Unit]
Description=Health check for $service_name
After=network.target

[Service]
Type=oneshot
ExecStart=/usr/local/bin/health-check-$service_name.sh "$service_name" "http://localhost:8080/health" 10
EOF

    cat > /etc/systemd/system/$service_name-healthcheck.timer << EOF
[Unit]
Description=Run health check for $service_name every 30 seconds
Requires=$service_name-healthcheck.service

[Timer]
OnBootSec=30
OnUnitActiveSec=30

[Install]
WantedBy=timers.target
EOF

    # å¯ç”¨å¥åº·æ£€æŸ¥å®šæ—¶å™¨
    systemctl daemon-reload
    systemctl enable $service_name-healthcheck.timer
    systemctl start $service_name-healthcheck.timer
    
    echo "âœ… Health checks configured for $service_name"
}

# å®æ–½æ•…éšœæ£€æµ‹
implement_failure_detection() {
    local service_name=$1
    
    echo "Implementing failure detection for $service_name"
    
    # åˆ›å»ºæ•…éšœæ£€æµ‹è„šæœ¬
    cat > /usr/local/bin/failure-detection-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
MAX_FAILURES="${2:-3}"
COOLDOWN_PERIOD="${3:-60}"

STATE_FILE="/var/run/$SERVICE_NAME-failure-state"
LOG_FILE="/var/log/$SERVICE_NAME-failure.log"

# åˆå§‹åŒ–çŠ¶æ€æ–‡ä»¶
if [ ! -f "$STATE_FILE" ]; then
    echo "0,0,$(date +%s)" > "$STATE_FILE"
fi

# è¯»å–å½“å‰çŠ¶æ€
read failures last_failure last_cooldown < "$STATE_FILE"

# æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
if /usr/local/bin/health-check-$SERVICE_NAME.sh "$SERVICE_NAME" "http://localhost:8080/health" 10; then
    # æœåŠ¡å¥åº·ï¼Œé‡ç½®å¤±è´¥è®¡æ•°ä½†ä¿æŒå†·å´æ—¶é—´
    echo "0,$last_failure,$last_cooldown" > "$STATE_FILE"
    echo "$(date): Service $SERVICE_NAME is healthy" >> "$LOG_FILE"
    exit 0
else
    # æœåŠ¡ä¸å¥åº·ï¼Œå¢åŠ å¤±è´¥è®¡æ•°
    current_time=$(date +%s)
    new_failures=$((failures + 1))
    echo "$new_failures,$current_time,$last_cooldown" > "$STATE_FILE"
    echo "$(date): Service $SERVICE_NAME failure detected (count: $new_failures)" >> "$LOG_FILE"
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ•…éšœé˜ˆå€¼
    if [ $new_failures -ge $MAX_FAILURES ]; then
        # æ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸå†…
        if [ $((current_time - last_cooldown)) -ge $COOLDOWN_PERIOD ]; then
            echo "$(date): Failure threshold reached for $SERVICE_NAME, triggering failover" >> "$LOG_FILE"
            # è§¦å‘æ•…éšœè½¬ç§»
            /usr/local/bin/trigger-failover-$SERVICE_NAME.sh
            # æ›´æ–°å†·å´æ—¶é—´
            echo "$new_failures,$current_time,$current_time" > "$STATE_FILE"
        else
            echo "$(date): In cooldown period, not triggering failover" >> "$LOG_FILE"
        fi
        exit 1
    fi
    exit 0
fi
EOF

    chmod +x /usr/local/bin/failure-detection-$service_name.sh
    
    # é…ç½®æ•…éšœæ£€æµ‹å®šæ—¶å™¨
    cat > /etc/systemd/system/$service_name-failure-detection.service << EOF
[Unit]
Description=Failure detection for $service_name
After=network.target

[Service]
Type=oneshot
ExecStart=/usr/local/bin/failure-detection-$service_name.sh "$service_name" 3 60
EOF

    cat > /etc/systemd/system/$service_name-failure-detection.timer << EOF
[Unit]
Description=Run failure detection for $service_name every 10 seconds
Requires=$service_name-failure-detection.service

[Timer]
OnBootSec=10
OnUnitActiveSec=10

[Install]
WantedBy=timers.target
EOF

    # å¯ç”¨æ•…éšœæ£€æµ‹å®šæ—¶å™¨
    systemctl daemon-reload
    systemctl enable $service_name-failure-detection.timer
    systemctl start $service_name-failure-detection.timer
    
    echo "âœ… Failure detection implemented for $service_name"
}

# é…ç½®æ•…éšœè½¬ç§»
configure_failover() {
    local service_name=$1
    
    echo "Configuring failover for $service_name"
    
    # åˆ›å»ºæ•…éšœè½¬ç§»è„šæœ¬
    cat > /usr/local/bin/trigger-failover-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
BACKUP_SERVICE="${2:-$SERVICE_NAME-backup}"

LOG_FILE="/var/log/$SERVICE_NAME-failover.log"

echo "$(date): Starting failover process for $SERVICE_NAME" >> "$LOG_FILE"

# åœæ­¢ä¸»æœåŠ¡
echo "$(date): Stopping primary service" >> "$LOG_FILE"
systemctl stop "$SERVICE_NAME" 2>> "$LOG_FILE"

# å¯åŠ¨å¤‡ä»½æœåŠ¡
echo "$(date): Starting backup service" >> "$LOG_FILE"
systemctl start "$BACKUP_SERVICE" 2>> "$LOG_FILE"

# æ£€æŸ¥å¤‡ä»½æœåŠ¡çŠ¶æ€
if systemctl is-active "$BACKUP_SERVICE" > /dev/null; then
    echo "$(date): Failover successful, backup service is running" >> "$LOG_FILE"
    
    # å‘é€å‘Šè­¦é€šçŸ¥
    echo "Failover triggered for $SERVICE_NAME at $(date)" | mail -s "Service Failover Alert" admin@example.com
    
    # è®°å½•æ•…éšœè½¬ç§»äº‹ä»¶
    echo "$(date): Failover completed successfully" >> "/var/log/service-failover-events.log"
    
    exit 0
else
    echo "$(date): Failover failed, backup service did not start" >> "$LOG_FILE"
    
    # å‘é€ç´§æ€¥å‘Šè­¦
    echo "CRITICAL: Failover failed for $SERVICE_NAME at $(date)" | mail -s "CRITICAL: Service Failover Failed" admin@example.com
    
    exit 1
fi
EOF

    chmod +x /usr/local/bin/trigger-failover-$service_name.sh
    
    echo "âœ… Failover configured for $service_name"
}

# è®¾ç½®æ¢å¤æœºåˆ¶
setup_recovery_mechanisms() {
    local service_name=$1
    
    echo "Setting up recovery mechanisms for $service_name"
    
    # åˆ›å»ºæœåŠ¡æ¢å¤è„šæœ¬
    cat > /usr/local/bin/recovery-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
PRIMARY_SERVICE="$SERVICE_NAME"
BACKUP_SERVICE="${SERVICE_NAME}-backup"

LOG_FILE="/var/log/$SERVICE_NAME-recovery.log"

echo "$(date): Starting recovery process for $SERVICE_NAME" >> "$LOG_FILE"

# æ£€æŸ¥ä¸»æœåŠ¡æ˜¯å¦å¯ä»¥æ¢å¤
if /usr/local/bin/can-recover-$SERVICE_NAME.sh; then
    echo "$(date): Primary service can be recovered" >> "$LOG_FILE"
    
    # åœæ­¢å¤‡ä»½æœåŠ¡
    echo "$(date): Stopping backup service" >> "$LOG_FILE"
    systemctl stop "$BACKUP_SERVICE" 2>> "$LOG_FILE"
    
    # å¯åŠ¨ä¸»æœåŠ¡
    echo "$(date): Starting primary service" >> "$LOG_FILE"
    systemctl start "$PRIMARY_SERVICE" 2>> "$LOG_FILE"
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    sleep 10
    
    # éªŒè¯ä¸»æœåŠ¡å¥åº·çŠ¶æ€
    if /usr/local/bin/health-check-$SERVICE_NAME.sh "$SERVICE_NAME" "http://localhost:8080/health" 10; then
        echo "$(date): Recovery successful, primary service is healthy" >> "$LOG_FILE"
        
        # å‘é€æ¢å¤é€šçŸ¥
        echo "Service $SERVICE_NAME recovered at $(date)" | mail -s "Service Recovery Notification" admin@example.com
        
        exit 0
    else
        echo "$(date): Recovery failed, primary service is still unhealthy" >> "$LOG_FILE"
        
        # é‡æ–°å¯åŠ¨å¤‡ä»½æœåŠ¡
        echo "$(date): Restarting backup service" >> "$LOG_FILE"
        systemctl start "$BACKUP_SERVICE" 2>> "$LOG_FILE"
        
        exit 1
    fi
else
    echo "$(date): Primary service cannot be recovered yet" >> "$LOG_FILE"
    exit 1
fi
EOF

    chmod +x /usr/local/bin/recovery-$service_name.sh
    
    # åˆ›å»ºæ¢å¤æ¡ä»¶æ£€æŸ¥è„šæœ¬
    cat > /usr/local/bin/can-recover-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"

# æ£€æŸ¥æ•…éšœæ ¹æœ¬åŸå› æ˜¯å¦å·²è§£å†³
# è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æ¢å¤æ¡ä»¶æ£€æŸ¥é€»è¾‘
# ä¾‹å¦‚ï¼šæ£€æŸ¥ä¾èµ–æœåŠ¡çŠ¶æ€ã€ç½‘ç»œè¿æ¥ã€èµ„æºé…ç½®ç­‰

echo "$(date): Checking recovery conditions for $SERVICE_NAME" >> "/var/log/$SERVICE_NAME-recovery-check.log"

# ç®€åŒ–ç¤ºä¾‹ï¼šæ€»æ˜¯å¯ä»¥æ¢å¤
exit 0
EOF

    chmod +x /usr/local/bin/can-recover-$service_name.sh
    
    echo "âœ… Recovery mechanisms set up for $service_name"
}

# éªŒè¯æ•…éšœè½¬ç§»æœºåˆ¶
verify_failover_mechanism() {
    local service_name=$1
    
    echo "Verifying failover mechanism for $service_name"
    
    # æ£€æŸ¥ç›¸å…³æœåŠ¡çŠ¶æ€
    echo "Checking systemd services:"
    systemctl status $service_name-healthcheck.timer 2>/dev/null || echo "Health check timer not found"
    systemctl status $service_name-failure-detection.timer 2>/dev/null || echo "Failure detection timer not found"
    
    # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
    echo "Checking scripts:"
    for script in health-check-$service_name.sh failure-detection-$service_name.sh trigger-failover-$service_name.sh recovery-$service_name.sh; do
        if [ -f "/usr/local/bin/$script" ]; then
            echo "âœ… $script exists"
        else
            echo "âŒ $script not found"
        fi
    done
    
    echo "âœ… Failover mechanism verification completed"
}

# ä½¿ç”¨ç¤ºä¾‹
# implement_failover_mechanism "config-service"
```

## å¥åº·æ£€æŸ¥ä¸è‡ªåŠ¨æ¢å¤ç­–ç•¥

å»ºç«‹å®Œå–„çš„å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨æ¢å¤ç­–ç•¥æ˜¯ç¡®ä¿é«˜å¯ç”¨æ€§ç³»ç»Ÿç¨³å®šè¿è¡Œçš„é‡è¦ä¿éšœã€‚

### 1. å¤šå±‚æ¬¡å¥åº·æ£€æŸ¥

```python
# health-check-framework.py
import requests
import subprocess
import psutil
import time
from typing import Dict, List, Any, Callable
from datetime import datetime
from enum import Enum

class HealthCheckType(Enum):
    HTTP = "http"
    TCP = "tcp"
    PROCESS = "process"
    DISK = "disk"
    MEMORY = "memory"
    CUSTOM = "custom"

class HealthStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"

class HealthCheckFramework:
    def __init__(self):
        self.checks = []
        self.results = []
        self.alert_handlers = []
        
    def add_health_check(self, name: str, check_type: HealthCheckType, 
                        check_func: Callable, threshold: float = 0.8,
                        critical: bool = False):
        """æ·»åŠ å¥åº·æ£€æŸ¥"""
        check = {
            'name': name,
            'type': check_type,
            'function': check_func,
            'threshold': threshold,
            'critical': critical,
            'last_result': None,
            'failure_count': 0
        }
        self.checks.append(check)
        print(f"âœ… Added health check: {name}")
        
    def run_health_checks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥"""
        print("ğŸƒ Running health checks...")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'total_checks': len(self.checks),
            'passed_checks': 0,
            'failed_checks': 0,
            'critical_failures': 0,
            'check_details': []
        }
        
        for check in self.checks:
            try:
                status, message = check['function']()
                check['last_result'] = {
                    'status': status,
                    'message': message,
                    'timestamp': datetime.now().isoformat()
                }
                
                check_result = {
                    'name': check['name'],
                    'type': check['type'].value,
                    'status': status.value,
                    'message': message
                }
                
                results['check_details'].append(check_result)
                
                if status == HealthStatus.HEALTHY:
                    results['passed_checks'] += 1
                    check['failure_count'] = 0
                else:
                    results['failed_checks'] += 1
                    check['failure_count'] += 1
                    
                    if check['critical']:
                        results['critical_failures'] += 1
                        
                    # è§¦å‘å‘Šè­¦
                    self.trigger_alerts(check, status, message)
                    
            except Exception as e:
                error_message = f"Health check execution failed: {str(e)}"
                check_result = {
                    'name': check['name'],
                    'type': check['type'].value,
                    'status': HealthStatus.UNKNOWN.value,
                    'message': error_message
                }
                results['check_details'].append(check_result)
                results['failed_checks'] += 1
                
        self.results.append(results)
        return results
        
    def trigger_alerts(self, check: Dict[str, Any], status: HealthStatus, message: str):
        """è§¦å‘å‘Šè­¦"""
        alert_info = {
            'check_name': check['name'],
            'status': status.value,
            'message': message,
            'failure_count': check['failure_count'],
            'timestamp': datetime.now().isoformat()
        }
        
        for handler in self.alert_handlers:
            try:
                handler(alert_info)
            except Exception as e:
                print(f"âŒ Alert handler failed: {e}")
                
    def add_alert_handler(self, handler: Callable):
        """æ·»åŠ å‘Šè­¦å¤„ç†å™¨"""
        self.alert_handlers.append(handler)
        print("âœ… Alert handler added")
        
    def get_system_health(self) -> HealthStatus:
        """è·å–ç³»ç»Ÿæ•´ä½“å¥åº·çŠ¶æ€"""
        if not self.results:
            return HealthStatus.UNKNOWN
            
        latest_result = self.results[-1]
        
        # å¦‚æœæœ‰å…³é”®æ•…éšœï¼Œç³»ç»Ÿä¸å¥åº·
        if latest_result['critical_failures'] > 0:
            return HealthStatus.UNHEALTHY
            
        # å¦‚æœå¤±è´¥æ£€æŸ¥è¶…è¿‡é˜ˆå€¼ï¼Œç³»ç»Ÿé™çº§
        failure_rate = latest_result['failed_checks'] / latest_result['total_checks']
        if failure_rate > 0.5:
            return HealthStatus.DEGRADED
            
        # å¦‚æœæœ‰å¤±è´¥æ£€æŸ¥ä½†ä¸è¶…è¿‡é˜ˆå€¼ï¼Œç³»ç»Ÿé™çº§
        if latest_result['failed_checks'] > 0:
            return HealthStatus.DEGRADED
            
        return HealthStatus.HEALTHY

# é¢„å®šä¹‰çš„å¥åº·æ£€æŸ¥å‡½æ•°
def http_health_check(url: str, expected_status: int = 200, timeout: int = 10) -> tuple:
    """HTTPå¥åº·æ£€æŸ¥"""
    try:
        response = requests.get(url, timeout=timeout)
        if response.status_code == expected_status:
            return (HealthStatus.HEALTHY, f"HTTP {response.status_code}")
        else:
            return (HealthStatus.DEGRADED, f"HTTP {response.status_code}, expected {expected_status}")
    except requests.exceptions.RequestException as e:
        return (HealthStatus.UNHEALTHY, f"HTTP request failed: {str(e)}")
        
def tcp_health_check(host: str, port: int, timeout: int = 5) -> tuple:
    """TCPç«¯å£å¥åº·æ£€æŸ¥"""
    import socket
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((host, port))
        sock.close()
        
        if result == 0:
            return (HealthStatus.HEALTHY, f"TCP port {port} is open")
        else:
            return (HealthStatus.UNHEALTHY, f"TCP port {port} is closed")
    except Exception as e:
        return (HealthStatus.UNHEALTHY, f"TCP check failed: {str(e)}")
        
def process_health_check(process_name: str) -> tuple:
    """è¿›ç¨‹å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
        for proc in psutil.process_iter(['pid', 'name']):
            if proc.info['name'] == process_name:
                return (HealthStatus.HEALTHY, f"Process {process_name} is running (PID: {proc.info['pid']})")
                
        return (HealthStatus.UNHEALTHY, f"Process {process_name} is not running")
    except Exception as e:
        return (HealthStatus.UNKNOWN, f"Process check failed: {str(e)}")
        
def disk_health_check(threshold: float = 80.0) -> tuple:
    """ç£ç›˜å¥åº·æ£€æŸ¥"""
    try:
        disk_usage = psutil.disk_usage('/')
        usage_percent = (disk_usage.used / disk_usage.total) * 100
        
        if usage_percent > threshold:
            return (HealthStatus.DEGRADED, f"Disk usage is high: {usage_percent:.1f}%")
        else:
            return (HealthStatus.HEALTHY, f"Disk usage is normal: {usage_percent:.1f}%")
    except Exception as e:
        return (HealthStatus.UNKNOWN, f"Disk check failed: {str(e)}")
        
def memory_health_check(threshold: float = 85.0) -> tuple:
    """å†…å­˜å¥åº·æ£€æŸ¥"""
    try:
        memory = psutil.virtual_memory()
        usage_percent = memory.percent
        
        if usage_percent > threshold:
            return (HealthStatus.DEGRADED, f"Memory usage is high: {usage_percent:.1f}%")
        else:
            return (HealthStatus.HEALTHY, f"Memory usage is normal: {usage_percent:.1f}%")
    except Exception as e:
        return (HealthStatus.UNKNOWN, f"Memory check failed: {str(e)}")

# å‘Šè­¦å¤„ç†å™¨ç¤ºä¾‹
def email_alert_handler(alert_info: Dict[str, Any]):
    """é‚®ä»¶å‘Šè­¦å¤„ç†å™¨"""
    print(f"ğŸ“§ Sending email alert: {alert_info}")
    # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„é‚®ä»¶å‘é€é€»è¾‘
    
def slack_alert_handler(alert_info: Dict[str, Any]):
    """Slackå‘Šè­¦å¤„ç†å™¨"""
    print(f"ğŸ’¬ Sending Slack alert: {alert_info}")
    # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„Slackæ¶ˆæ¯å‘é€é€»è¾‘
    
def log_alert_handler(alert_info: Dict[str, Any]):
    """æ—¥å¿—å‘Šè­¦å¤„ç†å™¨"""
    with open('/var/log/health-alerts.log', 'a') as f:
        f.write(f"{datetime.now().isoformat()}: {alert_info}\n")
    print(f"ğŸ“ Logged alert: {alert_info}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå¥åº·æ£€æŸ¥æ¡†æ¶
    health_framework = HealthCheckFramework()
    
    # æ·»åŠ å¥åº·æ£€æŸ¥
    health_framework.add_health_check(
        "config-service-health",
        HealthCheckType.HTTP,
        lambda: http_health_check("http://localhost:8080/health"),
        critical=True
    )
    
    health_framework.add_health_check(
        "database-connection",
        HealthCheckType.TCP,
        lambda: tcp_health_check("localhost", 5432),
        critical=True
    )
    
    health_framework.add_health_check(
        "config-process",
        HealthCheckType.PROCESS,
        lambda: process_health_check("config-service")
    )
    
    health_framework.add_health_check(
        "disk-usage",
        HealthCheckType.DISK,
        lambda: disk_health_check(85.0)
    )
    
    health_framework.add_health_check(
        "memory-usage",
        HealthCheckType.MEMORY,
        lambda: memory_health_check(90.0)
    )
    
    # æ·»åŠ å‘Šè­¦å¤„ç†å™¨
    health_framework.add_alert_handler(email_alert_handler)
    health_framework.add_alert_handler(slack_alert_handler)
    health_framework.add_alert_handler(log_alert_handler)
    
    # è¿è¡Œå¥åº·æ£€æŸ¥
    results = health_framework.run_health_checks()
    print(f"Health check results: {results}")
    
    # è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
    system_health = health_framework.get_system_health()
    print(f"System health status: {system_health.value}")
```

### 2. è‡ªåŠ¨æ¢å¤æœºåˆ¶

```bash
# auto-recovery.sh

# è‡ªåŠ¨æ¢å¤æœºåˆ¶è„šæœ¬
implement_auto_recovery() {
    local service_name=${1:-"config-service"}
    
    echo "Implementing auto recovery for $service_name"
    
    # 1. é…ç½®è‡ªåŠ¨æ¢å¤æ£€æŸ¥
    echo "Step 1: Configuring auto recovery checks"
    configure_auto_recovery_checks "$service_name"
    
    # 2. å®æ–½è‡ªåŠ¨æ¢å¤ç­–ç•¥
    echo "Step 2: Implementing auto recovery strategies"
    implement_recovery_strategies "$service_name"
    
    # 3. è®¾ç½®æ¢å¤ç›‘æ§
    echo "Step 3: Setting up recovery monitoring"
    setup_recovery_monitoring "$service_name"
    
    # 4. éªŒè¯è‡ªåŠ¨æ¢å¤
    echo "Step 4: Verifying auto recovery mechanism"
    verify_auto_recovery "$service_name"
    
    echo "Auto recovery implementation completed"
}

# é…ç½®è‡ªåŠ¨æ¢å¤æ£€æŸ¥
configure_auto_recovery_checks() {
    local service_name=$1
    
    echo "Configuring auto recovery checks for $service_name"
    
    # åˆ›å»ºæ¢å¤æ¡ä»¶æ£€æŸ¥è„šæœ¬
    cat > /usr/local/bin/check-recovery-conditions-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
LOG_FILE="/var/log/$SERVICE_NAME-recovery-check.log"

echo "$(date): Checking recovery conditions for $SERVICE_NAME" >> "$LOG_FILE"

# æ£€æŸ¥åŸºæœ¬æ¡ä»¶
RECOVERY_POSSIBLE=true
REASONS=()

# æ£€æŸ¥ä¾èµ–æœåŠ¡
if ! systemctl is-active database.service >/dev/null 2>&1; then
    RECOVERY_POSSIBLE=false
    REASONS+=("Database service is not running")
    echo "$(date): Database service is not running" >> "$LOG_FILE"
fi

# æ£€æŸ¥ç½‘ç»œè¿æ¥
if ! ping -c 1 8.8.8.8 >/dev/null 2>&1; then
    RECOVERY_POSSIBLE=false
    REASONS+=("Network connectivity issue")
    echo "$(date): Network connectivity issue detected" >> "$LOG_FILE"
fi

# æ£€æŸ¥ç£ç›˜ç©ºé—´
DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
if [ "$DISK_USAGE" -gt 90 ]; then
    RECOVERY_POSSIBLE=false
    REASONS+=("Disk space is critically low")
    echo "$(date): Disk space critically low ($DISK_USAGE%)" >> "$LOG_FILE"
fi

# æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
MEMORY_USAGE=$(free | grep Mem | awk '{printf("%.0f", $3/$2 * 100.0)}')
if [ "$MEMORY_USAGE" -gt 95 ]; then
    RECOVERY_POSSIBLE=false
    REASONS+=("Memory usage is critically high")
    echo "$(date): Memory usage critically high ($MEMORY_USAGE%)" >> "$LOG_FILE"
fi

if [ "$RECOVERY_POSSIBLE" = true ]; then
    echo "$(date): Recovery conditions are met" >> "$LOG_FILE"
    echo "RECOVERY_POSSIBLE"
else
    echo "$(date): Recovery conditions not met: ${REASONS[*]}" >> "$LOG_FILE"
    echo "RECOVERY_NOT_POSSIBLE"
fi
EOF

    chmod +x /usr/local/bin/check-recovery-conditions-$service_name.sh
    
    echo "âœ… Auto recovery checks configured for $service_name"
}

# å®æ–½æ¢å¤ç­–ç•¥
implement_recovery_strategies() {
    local service_name=$1
    
    echo "Implementing recovery strategies for $service_name"
    
    # åˆ›å»ºæœåŠ¡é‡å¯ç­–ç•¥
    cat > /usr/local/bin/recovery-strategy-restart-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
LOG_FILE="/var/log/$SERVICE_NAME-recovery.log"

echo "$(date): Executing restart recovery strategy for $SERVICE_NAME" >> "$LOG_FILE"

# åœæ­¢æœåŠ¡
echo "$(date): Stopping service" >> "$LOG_FILE"
systemctl stop "$SERVICE_NAME" 2>> "$LOG_FILE"

# ç­‰å¾…æœåŠ¡å®Œå…¨åœæ­¢
sleep 5

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
echo "$(date): Cleaning temporary files" >> "$LOG_FILE"
rm -rf /tmp/$SERVICE_NAME-* 2>> "$LOG_FILE"
rm -rf /var/run/$SERVICE_NAME.pid 2>> "$LOG_FILE"

# é‡å¯æœåŠ¡
echo "$(date): Starting service" >> "$LOG_FILE"
systemctl start "$SERVICE_NAME" 2>> "$LOG_FILE"

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 10

# éªŒè¯æœåŠ¡çŠ¶æ€
if systemctl is-active "$SERVICE_NAME" >/dev/null 2>&1; then
    echo "$(date): Service restarted successfully" >> "$LOG_FILE"
    exit 0
else
    echo "$(date): Service restart failed" >> "$LOG_FILE"
    exit 1
fi
EOF

    chmod +x /usr/local/bin/recovery-strategy-restart-$service_name.sh
    
    # åˆ›å»ºé…ç½®é‡è½½ç­–ç•¥
    cat > /usr/local/bin/recovery-strategy-reload-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
LOG_FILE="/var/log/$SERVICE_NAME-recovery.log"

echo "$(date): Executing reload recovery strategy for $SERVICE_NAME" >> "$LOG_FILE"

# é‡æ–°åŠ è½½é…ç½®
echo "$(date): Reloading configuration" >> "$LOG_FILE"
systemctl reload "$SERVICE_NAME" 2>> "$LOG_FILE"

# ç­‰å¾…é‡è½½å®Œæˆ
sleep 5

# éªŒè¯æœåŠ¡çŠ¶æ€
if systemctl is-active "$SERVICE_NAME" >/dev/null 2>&1; then
    echo "$(date): Configuration reloaded successfully" >> "$LOG_FILE"
    exit 0
else
    echo "$(date): Configuration reload failed" >> "$LOG_FILE"
    exit 1
fi
EOF

    chmod +x /usr/local/bin/recovery-strategy-reload-$service_name.sh
    
    # åˆ›å»ºå®Œæ•´æ¢å¤ç­–ç•¥
    cat > /usr/local/bin/recovery-strategy-full-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
LOG_FILE="/var/log/$SERVICE_NAME-recovery.log"

echo "$(date): Executing full recovery strategy for $SERVICE_NAME" >> "$LOG_FILE"

# åœæ­¢æœåŠ¡
echo "$(date): Stopping service" >> "$LOG_FILE"
systemctl stop "$SERVICE_NAME" 2>> "$LOG_FILE"

# å¤‡ä»½å½“å‰é…ç½®
echo "$(date): Backing up current configuration" >> "$LOG_FILE"
cp -r /etc/$SERVICE_NAME /etc/$SERVICE_NAME.backup.$(date +%Y%m%d%H%M%S) 2>> "$LOG_FILE"

# æ¢å¤é»˜è®¤é…ç½®
echo "$(date): Restoring default configuration" >> "$LOG_FILE"
cp -r /etc/$SERVICE_NAME.default/* /etc/$SERVICE_NAME/ 2>> "$LOG_FILE"

# æ¸…ç†æ•°æ®
echo "$(date): Cleaning service data" >> "$LOG_FILE"
rm -rf /var/lib/$SERVICE_NAME/* 2>> "$LOG_FILE"

# é‡å¯æœåŠ¡
echo "$(date): Starting service with default configuration" >> "$LOG_FILE"
systemctl start "$SERVICE_NAME" 2>> "$LOG_FILE"

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 15

# éªŒè¯æœåŠ¡çŠ¶æ€
if systemctl is-active "$SERVICE_NAME" >/dev/null 2>&1; then
    echo "$(date): Full recovery completed successfully" >> "$LOG_FILE"
    exit 0
else
    echo "$(date): Full recovery failed" >> "$LOG_FILE"
    exit 1
fi
EOF

    chmod +x /usr/local/bin/recovery-strategy-full-$service_name.sh
    
    echo "âœ… Recovery strategies implemented for $service_name"
}

# è®¾ç½®æ¢å¤ç›‘æ§
setup_recovery_monitoring() {
    local service_name=$1
    
    echo "Setting up recovery monitoring for $service_name"
    
    # åˆ›å»ºè‡ªåŠ¨æ¢å¤ç›‘æ§è„šæœ¬
    cat > /usr/local/bin/auto-recovery-monitor-$service_name.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="$1"
MAX_RECOVERY_ATTEMPTS="${2:-3}"
COOLDOWN_PERIOD="${3:-300}"  # 5åˆ†é’Ÿå†·å´æœŸ

STATE_FILE="/var/run/$SERVICE_NAME-recovery-state"
LOG_FILE="/var/log/$SERVICE_NAME-auto-recovery.log"

# åˆå§‹åŒ–çŠ¶æ€æ–‡ä»¶
if [ ! -f "$STATE_FILE" ]; then
    echo "0,0,$(date +%s)" > "$STATE_FILE"
fi

# è¯»å–å½“å‰çŠ¶æ€
read recovery_attempts last_recovery last_cooldown < "$STATE_FILE"

echo "$(date): Auto recovery check for $SERVICE_NAME" >> "$LOG_FILE"
echo "$(date): Current recovery attempts: $recovery_attempts" >> "$LOG_FILE"

# æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
if /usr/local/bin/health-check-$SERVICE_NAME.sh "$SERVICE_NAME" "http://localhost:8080/health" 10; then
    # æœåŠ¡å¥åº·ï¼Œé‡ç½®æ¢å¤å°è¯•è®¡æ•°
    if [ $recovery_attempts -gt 0 ]; then
        echo "$(date): Service is healthy, resetting recovery attempts" >> "$LOG_FILE"
        echo "0,0,$last_cooldown" > "$STATE_FILE"
    fi
    exit 0
fi

echo "$(date): Service is unhealthy, checking recovery conditions" >> "$LOG_FILE"

# æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤
RECOVERY_CONDITIONS=$(/usr/local/bin/check-recovery-conditions-$SERVICE_NAME.sh "$SERVICE_NAME")
if [ "$RECOVERY_CONDITIONS" != "RECOVERY_POSSIBLE" ]; then
    echo "$(date): Recovery conditions not met, skipping auto recovery" >> "$LOG_FILE"
    exit 1
fi

# æ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸå†…
current_time=$(date +%s)
if [ $((current_time - last_cooldown)) -lt $COOLDOWN_PERIOD ]; then
    echo "$(date): In cooldown period, skipping auto recovery" >> "$LOG_FILE"
    exit 1
fi

# æ£€æŸ¥æ¢å¤å°è¯•æ¬¡æ•°
if [ $recovery_attempts -ge $MAX_RECOVERY_ATTEMPTS ]; then
    echo "$(date): Maximum recovery attempts reached, manual intervention required" >> "$LOG_FILE"
    
    # å‘é€ç´§æ€¥å‘Šè­¦
    echo "CRITICAL: Maximum recovery attempts reached for $SERVICE_NAME" | \
        mail -s "CRITICAL: Service Recovery Failed" admin@example.com
        
    exit 1
fi

# æ‰§è¡Œè‡ªåŠ¨æ¢å¤
new_recovery_attempts=$((recovery_attempts + 1))
echo "$(date): Attempting auto recovery (attempt $new_recovery_attempts)" >> "$LOG_FILE"

# æ ¹æ®å¤±è´¥æ¬¡æ•°é€‰æ‹©æ¢å¤ç­–ç•¥
RECOVERY_SUCCESS=false
if [ $new_recovery_attempts -eq 1 ]; then
    # ç¬¬ä¸€æ¬¡å°è¯•ï¼šé‡å¯æœåŠ¡
    if /usr/local/bin/recovery-strategy-restart-$SERVICE_NAME.sh "$SERVICE_NAME"; then
        RECOVERY_SUCCESS=true
    fi
elif [ $new_recovery_attempts -eq 2 ]; then
    # ç¬¬äºŒæ¬¡å°è¯•ï¼šé‡æ–°åŠ è½½é…ç½®
    if /usr/local/bin/recovery-strategy-reload-$SERVICE_NAME.sh "$SERVICE_NAME"; then
        RECOVERY_SUCCESS=true
    fi
else
    # ç¬¬ä¸‰æ¬¡å°è¯•ï¼šå®Œæ•´æ¢å¤
    if /usr/local/bin/recovery-strategy-full-$SERVICE_NAME.sh "$SERVICE_NAME"; then
        RECOVERY_SUCCESS=true
    fi
fi

if [ "$RECOVERY_SUCCESS" = true ]; then
    echo "$(date): Auto recovery successful" >> "$LOG_FILE"
    # é‡ç½®æ¢å¤å°è¯•è®¡æ•°ï¼Œä½†æ›´æ–°æœ€åæ¢å¤æ—¶é—´
    echo "0,$current_time,$current_time" > "$STATE_FILE"
    
    # å‘é€æ¢å¤æˆåŠŸé€šçŸ¥
    echo "Service $SERVICE_NAME auto recovery successful at $(date)" | \
        mail -s "Service Recovery Success" admin@example.com
        
    exit 0
else
    echo "$(date): Auto recovery failed" >> "$LOG_FILE"
    # æ›´æ–°æ¢å¤å°è¯•è®¡æ•°å’Œæ—¶é—´
    echo "$new_recovery_attempts,$current_time,$last_cooldown" > "$STATE_FILE"
    
    # å¦‚æœè¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œå‘é€ç´§æ€¥å‘Šè­¦
    if [ $new_recovery_attempts -ge $MAX_RECOVERY_ATTEMPTS ]; then
        echo "CRITICAL: Auto recovery failed after $MAX_RECOVERY_ATTEMPTS attempts for $SERVICE_NAME" | \
            mail -s "CRITICAL: Service Recovery Failed" admin@example.com
    fi
    
    exit 1
fi
EOF

    chmod +x /usr/local/bin/auto-recovery-monitor-$service_name.sh
    
    # é…ç½®è‡ªåŠ¨æ¢å¤å®šæ—¶å™¨
    cat > /etc/systemd/system/$service_name-auto-recovery.service << EOF
[Unit]
Description=Auto recovery for $service_name
After=network.target

[Service]
Type=oneshot
ExecStart=/usr/local/bin/auto-recovery-monitor-$service_name.sh "$service_name" 3 300
EOF

    cat > /etc/systemd/system/$service_name-auto-recovery.timer << EOF
[Unit]
Description=Run auto recovery for $service_name every 60 seconds
Requires=$service_name-auto-recovery.service

[Timer]
OnBootSec=60
OnUnitActiveSec=60

[Install]
WantedBy=timers.target
EOF

    # å¯ç”¨è‡ªåŠ¨æ¢å¤å®šæ—¶å™¨
    systemctl daemon-reload
    systemctl enable $service_name-auto-recovery.timer
    systemctl start $service_name-auto-recovery.timer
    
    echo "âœ… Recovery monitoring set up for $service_name"
}

# éªŒè¯è‡ªåŠ¨æ¢å¤
verify_auto_recovery() {
    local service_name=$1
    
    echo "Verifying auto recovery for $service_name"
    
    # æ£€æŸ¥ç›¸å…³æ–‡ä»¶
    echo "Checking recovery scripts:"
    for script in check-recovery-conditions-$service_name.sh recovery-strategy-restart-$service_name.sh recovery-strategy-reload-$service_name.sh recovery-strategy-full-$service_name.sh auto-recovery-monitor-$service_name.sh; do
        if [ -f "/usr/local/bin/$script" ]; then
            echo "âœ… $script exists"
        else
            echo "âŒ $script not found"
        fi
    done
    
    # æ£€æŸ¥systemdæœåŠ¡
    echo "Checking systemd services:"
    systemctl status $service_name-auto-recovery.timer 2>/dev/null || echo "Auto recovery timer not found"
    
    echo "âœ… Auto recovery verification completed"
}

# ä½¿ç”¨ç¤ºä¾‹
# implement_auto_recovery "config-service"
```

## æœ€ä½³å®è·µæ€»ç»“

é€šè¿‡ä»¥ä¸Šå†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥æ€»ç»“å‡ºé«˜å¯ç”¨æ€§ç¯å¢ƒä¸­é…ç½®ç®¡ç†ç­–ç•¥çš„æœ€ä½³å®è·µï¼š

### 1. æ¶æ„è®¾è®¡åŸåˆ™
- éµå¾ªå†—ä½™è®¾è®¡åŸåˆ™ï¼Œæ¶ˆé™¤å•ç‚¹æ•…éšœ
- å®æ–½æ•…éšœéš”ç¦»æœºåˆ¶ï¼Œé™åˆ¶æ•…éšœå½±å“èŒƒå›´
- è®¾è®¡ä¼˜é›…é™çº§æ–¹æ¡ˆï¼Œç¡®ä¿æ ¸å¿ƒåŠŸèƒ½å¯ç”¨

### 2. é«˜å¯ç”¨éƒ¨ç½²
- é‡‡ç”¨é›†ç¾¤åŒ–éƒ¨ç½²é…ç½®å­˜å‚¨ç»„ä»¶
- å®æ–½è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»æœºåˆ¶
- é…ç½®å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨æ¢å¤ç­–ç•¥

### 3. è´Ÿè½½å‡è¡¡ç­–ç•¥
- é€‰æ‹©åˆé€‚çš„è´Ÿè½½å‡è¡¡ç®—æ³•
- å®æ–½æ™ºèƒ½æ•…éšœæ£€æµ‹æœºåˆ¶
- é…ç½®åˆç†çš„å†·å´æœŸå’Œé‡è¯•ç­–ç•¥

### 4. å¥åº·æ£€æŸ¥ä¸æ¢å¤
- å»ºç«‹å¤šå±‚æ¬¡å¥åº·æ£€æŸ¥ä½“ç³»
- å®æ–½è‡ªåŠ¨åŒ–æ¢å¤æœºåˆ¶
- é…ç½®ç›‘æ§å‘Šè­¦å’Œæ—¥å¿—è®°å½•

é€šè¿‡å®æ–½è¿™äº›æœ€ä½³å®è·µï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªç¨³å®šå¯é çš„é«˜å¯ç”¨æ€§é…ç½®ç®¡ç†ä½“ç³»ï¼Œç¡®ä¿åœ¨å„ç§æ•…éšœåœºæ™¯ä¸‹ç³»ç»Ÿä»èƒ½æŒç»­æä¾›æœåŠ¡ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨é…ç½®ç®¡ç†çš„å†—ä½™ä¸å®¹é”™è®¾è®¡ï¼Œå¸®åŠ©æ‚¨æŒæ¡æ›´åŠ å…ˆè¿›çš„é«˜å¯ç”¨æ€§é…ç½®ç®¡ç†æŠ€æœ¯ã€‚